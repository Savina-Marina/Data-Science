# UNIX Command Line Tools for Data Processing

## Описание проекта

Проект посвящён практической работе с UNIX-инструментами командной строки для сбора, обработки и первичного анализа данных.  
В рамках проекта был реализован полный мини-пайплайн обработки данных вакансий: от получения данных через API до очистки, сортировки, агрегации и разбиения по датам.

Проект выполнен в формате набора shell-скриптов с чётко заданными ограничениями на используемые инструменты, что демонстрирует умение работать в условиях реальных технических требований.

## Источник данных

В качестве источника данных использовалось публичное API HeadHunter.  
Данные о вакансиях по ключевому слову `data scientist` загружались в формате JSON и далее преобразовывались для аналитических задач.

## Используемые технологии и инструменты

- UNIX shell (`/bin/sh`)
- `curl` — получение данных из HTTP API
- `jq` — парсинг и трансформация JSON
- `sort`, `uniq`, `head`, `tail`, `cat` — обработка CSV файлов
- `sed` и стандартные утилиты shell — очистка и преобразование данных
- CSV как основной формат для аналитической обработки
- Git для контроля версий

## Структура проекта и выполненные задачи

### Exercise 00 — Получение данных
- Реализован shell-скрипт для запроса данных к HeadHunter API
- Загружены первые 20 вакансий по заданному ключевому слову
- Результат сохранён в JSON-файл с форматированием «одно поле — одна строка»

### Exercise 01 — Преобразование JSON в CSV
- С помощью `jq` данные преобразованы из JSON в CSV
- Выделены ключевые поля вакансий:  
  `id`, `created_at`, `name`, `has_test`, `alternate_url`
- Сформирован CSV с заголовками

### Exercise 02 — Сортировка данных
- CSV-файл отсортирован по дате создания вакансии
- Вторичный ключ сортировки — идентификатор вакансии
- Заголовки сохранены

### Exercise 03 — Очистка текстовых данных
- Из названий вакансий извлечены уровни: `Junior`, `Middle`, `Senior`
- Удалён информационный шум
- Подготовленные данные сохранены в новом CSV-файле
- Сохранён порядок сортировки

### Exercise 04 — Описательная статистика
- Подсчитано количество вакансий по каждому уровню
- Выполнена сортировка по убыванию количества
- Получен агрегированный CSV с частотами значений

### Exercise 05 — Партиционирование и конкатенация
- Данные разделены на отдельные CSV-файлы по дате публикации
- Реализована обратная сборка файлов в единый CSV
- Проверена идентичность результата исходным данным

## Что демонстрирует проект

- Умение работать с данными без GUI и специализированных библиотек
- Понимание этапов data-pipeline: сбор → очистка → трансформация → агрегация
- Навыки обработки реальных API-данных
- Аккуратная работа с форматами JSON и CSV
- Следование строгим техническим требованиям и ограничениям
- Чистая структура проекта и воспроизводимость результатов

## Формат выполнения

Проект полностью воспроизводим в любой UNIX-совместимой среде и не требует сторонних библиотек или языков программирования, кроме стандартных CLI-утилит.
